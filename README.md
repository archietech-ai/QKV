# QKV

## Simple Self attention

We want to use a query matrix and multiply that in key matrices. In a simple term it will be like:
![image](https://github.com/user-attachments/assets/8f71f2cb-e96d-49bd-a688-679ad6e1c5a9)

But in a bit more realistic way first we calculate the "attention weigh". I have shown that in two steps:

### Step 1:


![image](https://github.com/user-attachments/assets/3183ec2c-7f6f-4ba5-8847-cefcf81af34b)

### Step 2:

![image](https://github.com/user-attachments/assets/de84c53b-cf77-4632-a28f-4a1407c83194)


## Self attention

Now we want to perform real self-attention like the way it is implemented in the original transformer.

![image](https://github.com/user-attachments/assets/04e00326-fc32-4a72-8776-5ee10ad15768)


![image](https://github.com/user-attachments/assets/fdf48328-0a0c-422d-bdea-d2e1ed6faeb4)


## Single Head 
![LLM-Page-3 drawio](https://github.com/user-attachments/assets/e0e1ff14-707f-4614-a551-6bc3cca47208)





